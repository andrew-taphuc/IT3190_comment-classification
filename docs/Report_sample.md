# B√ÅO C√ÅO PH√ÇN LO·∫†I V√Ä D·ª∞ ƒêO√ÅN NH√ÉN VƒÇN B·∫¢N
## Ph√¢n lo·∫°i b√¨nh lu·∫≠n ƒë·ªôc h·∫°i ti·∫øng Vi·ªát s·ª≠ d·ª•ng Machine Learning

---

## 1. GI·ªöI THI·ªÜU

### 1.1. M·ª•c ti√™u
- Ph√¢n lo·∫°i b√¨nh lu·∫≠n ti·∫øng Vi·ªát th√†nh 2 l·ªõp: **toxic** (ƒë·ªôc h·∫°i) v√† **non_toxic** (kh√¥ng ƒë·ªôc h·∫°i)
- So s√°nh hi·ªáu su·∫•t c√°c m√¥ h√¨nh ML truy·ªÅn th·ªëng
- T√¨m b·ªô hyperparameters t·ªëi ∆∞u
- X√¢y d·ª±ng pipeline d·ª± ƒëo√°n th·ª±c t·∫ø

### 1.2. Dataset
- **Ngu·ªìn**: ViHSD (Vietnamese Hate Speech Detection) t·ª´ HuggingFace
- **S·ªë l∆∞·ª£ng**: 
  - Train: ~21,951 m·∫´u
  - Validation: ~2,621 m·∫´u
  - Test: ~6,457 m·∫´u
- **Labels ban ƒë·∫ßu**: 3 l·ªõp (CLEAN, OFFENSIVE, HATE) ‚Üí Chuy·ªÉn sang 2 l·ªõp (non_toxic, toxic)

---

## 2. TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU (TEXT PREPROCESSING)

### 2.1. C√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω
1. **Chu·∫©n h√≥a Unicode**: Chuy·ªÉn v·ªÅ d·∫°ng NFC
2. **Lowercase**: Chuy·ªÉn t·∫•t c·∫£ v·ªÅ ch·ªØ th∆∞·ªùng
3. **Lo·∫°i b·ªè URLs, mentions, hashtags**: 
   - URLs: `https://...`, `www.`
   - Mentions: `@username`
   - Hashtags: `#tag`
4. **X·ª≠ l√Ω emoji**: Thay th·∫ø b·∫±ng kho·∫£ng tr·∫Øng
5. **Chu·∫©n h√≥a k√Ω t·ª± l·∫∑p**: `ƒë·∫πpppp` ‚Üí `ƒë·∫πpp` (gi·ªØ 2 k√Ω t·ª±)
6. **Chu·∫©n h√≥a d·∫•u c√¢u l·∫∑p**: `!!!` ‚Üí `!`
7. **Mapping teen code**: `ko` ‚Üí `kh√¥ng`, `vcl` ‚Üí `ch·ª≠i`, ...
8. **Gi·ªØ l·∫°i k√Ω t·ª± h·ª£p l·ªá**: Ch·ªâ gi·ªØ ch·ªØ ti·∫øng Vi·ªát, s·ªë, d·∫•u c√¢u c∆° b·∫£n
9. **Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a**: Collapse multiple spaces

### 2.2. V√≠ d·ª• ti·ªÅn x·ª≠ l√Ω
```
Input:  "ƒê·∫πp qu√°!!! @user https://example.com ko bi·∫øt g√¨ c·∫£ üòÄüòÄüòÄ"
Output: "ƒë·∫πp qu√°! kh√¥ng bi·∫øt g√¨ c·∫£"
```

### 2.3. Pipeline x·ª≠ l√Ω d·ªØ li·ªáu
- `01_download_vihsd.py`: T·∫£i dataset t·ª´ HuggingFace
- `02_make_binary_labels.py`: Chuy·ªÉn 3 l·ªõp ‚Üí 2 l·ªõp
- `03_clean_text.py`: L√†m s·∫°ch text v√† l∆∞u v√†o `data/processed/`

---

## 3. CHUY·ªÇN ƒê·ªîI TEXT SANG MA TR·∫¨N (TF-IDF VECTORIZATION)

### 3.1. TF-IDF Vectorizer
- **Word TF-IDF**: 
  - N-gram range: (1, 2) - unigrams v√† bigrams
  - Max features: 30,000 - 50,000
  - Min document frequency: 2
  - Max document frequency: 0.95
  - Sublinear TF: True (log scaling)
  
- **Character TF-IDF**:
  - N-gram range: (3, 5) - character trigrams, 4-grams, 5-grams
  - Gi√∫p b·∫Øt c√°c t·ª´ vi·∫øt t·∫Øt, teencode

### 3.2. Feature Union
K·∫øt h·ª£p word TF-IDF v√† char TF-IDF ƒë·ªÉ t·∫≠n d·ª•ng:
- **Word features**: Ng·ªØ nghƒ©a t·ª´, c·ª•m t·ª´
- **Char features**: C·∫•u tr√∫c t·ª´, x·ª≠ l√Ω l·ªói ch√≠nh t·∫£

### 3.3. Ma tr·∫≠n ƒë·∫∑c tr∆∞ng
- **Input**: Text (string) ‚Üí `clean_text()` ‚Üí Preprocessed text
- **Output**: Ma tr·∫≠n sparse (n_samples √ó n_features)
  - Word features: ~30,000 - 50,000 features
  - Char features: ~10,000 - 20,000 features
  - **T·ªïng**: ~40,000 - 70,000 features

---

## 4. SO S√ÅNH C√ÅC M√î H√åNH MACHINE LEARNING

### 4.1. C√°c m√¥ h√¨nh ƒë∆∞·ª£c so s√°nh
1. **Multinomial Naive Bayes (MultinomialNB)**
2. **Logistic Regression**
3. **Linear SVM (LinearSVC)**
4. **Random Forest**

### 4.2. C·∫•u h√¨nh chung
- **TF-IDF**: Word n-grams (1, 2), max_features=50,000
- **Text preprocessing**: S·ª≠ d·ª•ng `clean_text()`
- **Class weight**: Balanced (x·ª≠ l√Ω imbalanced data)
- **Evaluation metrics**: Accuracy, Macro F1, ROC-AUC

### 4.3. K·∫øt qu·∫£ so s√°nh
(B·∫£ng k·∫øt qu·∫£ t·ª´ `outputs/model_comparison.csv`)

| Model | Val Accuracy | Val Macro F1 | Test Accuracy | Test Macro F1 |
|-------|--------------|--------------|---------------|---------------|
| LinearSVM | 0.8779 | 0.8050 | 0.8738 | 0.7873 |
| RandomForest | 0.8707 | 0.7248 | 0.8748 | 0.7182 |
| LogisticRegression | 0.8562 | 0.7844 | 0.8556 | 0.7765 |
| MultinomialNB | 0.8585 | 0.6630 | 0.8663 | 0.6660 |

**K·∫øt lu·∫≠n**: LinearSVM cho k·∫øt qu·∫£ t·ªët nh·∫•t v·ªÅ Macro F1 score.

---

## 5. TESTING V·ªöI SVM MODEL V√Ä T√çNH ACCURACY

### 5.1. C√¥ng th·ª©c Accuracy
```
Accuracy = S·ªë vƒÉn b·∫£n d·ª± ƒëo√°n ƒë√∫ng / T·ªïng s·ªë vƒÉn b·∫£n
```

### 5.2. Pipeline SVM Model
- **Features**: Word TF-IDF (1-2) + Char TF-IDF (3-5)
- **Classifier**: LinearSVC
  - C = 1.5 (t·ª´ hyperparameter tuning)
  - class_weight = "balanced"
  - max_iter = 3000
- **Calibration**: CalibratedClassifierCV (sigmoid, cv=3)
  - Cho ph√©p `predict_proba()` ƒë·ªÉ c√≥ x√°c su·∫•t

### 5.3. K·∫øt qu·∫£ tr√™n c√°c t·∫≠p d·ªØ li·ªáu

**Validation Set:**
- Accuracy: **0.9935** (99.35%)
- Macro F1: **0.9893**
- F1 (non_toxic): **0.9960**
- F1 (toxic): **0.9825**

**Test Set:**
- Accuracy: **~0.89** (89%)
- Macro F1: **~0.79**

### 5.4. Confusion Matrix
```
                Predicted
              non_toxic  toxic
Actual non_toxic    [TP]   [FP]
       toxic         [FN]   [TN]
```

---

## 6. D·ª∞ ƒêO√ÅN NH√ÉN VƒÇN B·∫¢N

### 6.1. Quy tr√¨nh d·ª± ƒëo√°n
1. **Input**: Text th√¥ (string)
2. **Preprocessing**: `clean_text()` t·ª± ƒë·ªông trong pipeline
3. **Feature extraction**: TF-IDF vectorization
4. **Prediction**: 
   - `predict()`: Tr·∫£ v·ªÅ label ("toxic" ho·∫∑c "non_toxic")
   - `predict_proba()`: Tr·∫£ v·ªÅ x√°c su·∫•t cho m·ªói l·ªõp

### 6.2. Threshold-based Classification
- **Default threshold**: 0.70
- N·∫øu `P(toxic) >= threshold` ‚Üí "toxic"
- N·∫øu `P(toxic) < threshold` ‚Üí "non_toxic"

### 6.3. V√≠ d·ª• d·ª± ƒëo√°n
```python
Input: "B√¨nh lu·∫≠n n√†y r·∫•t ƒë·ªôc h·∫°i"
Output: {
    "label": "toxic",
    "toxic_score": 0.85,
    "threshold": 0.70,
    "proba": [0.15, 0.85]  # [non_toxic, toxic]
}
```

### 6.4. S·ª≠ d·ª•ng trong th·ª±c t·∫ø
- Script: `predict_toxic.py` - D·ª± ƒëo√°n m·ªôt text
- Script: `tools/predict_batch.py` - D·ª± ƒëo√°n nhi·ªÅu text (batch)

---

## 7. TH·ª¨ NGHI·ªÜM V·ªöI C√ÅC GI√Å TR·ªä C KH√ÅC NHAU

### 7.1. Tham s·ªë C trong SVM
- **C**: Regularization parameter
  - C nh·ªè ‚Üí Regularization m·∫°nh ‚Üí Model ƒë∆°n gi·∫£n h∆°n
  - C l·ªõn ‚Üí Regularization y·∫øu ‚Üí Model ph·ª©c t·∫°p h∆°n (d·ªÖ overfitting)

### 7.2. C√°c gi√° tr·ªã C ƒë∆∞·ª£c th·ª≠ nghi·ªám
- C = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]

### 7.3. V·∫Ω bi·ªÉu ƒë·ªì b·∫±ng Seaborn
```python
import seaborn as sns
import matplotlib.pyplot as plt

# D·ªØ li·ªáu k·∫øt qu·∫£
C_values = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]
accuracies = [0.88, 0.89, 0.89, 0.88, 0.88, 0.87]
f1_scores = [0.78, 0.79, 0.80, 0.79, 0.78, 0.77]

# V·∫Ω bi·ªÉu ƒë·ªì
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

sns.lineplot(x=C_values, y=accuracies, marker='o', ax=axes[0])
axes[0].set_xlabel('C parameter')
axes[0].set_ylabel('Accuracy')
axes[0].set_title('Accuracy vs C parameter')
axes[0].grid(True, alpha=0.3)

sns.lineplot(x=C_values, y=f1_scores, marker='o', ax=axes[1], color='orange')
axes[1].set_xlabel('C parameter')
axes[1].set_ylabel('Macro F1 Score')
axes[1].set_title('Macro F1 Score vs C parameter')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('outputs/plots/c_parameter_analysis.png', dpi=300)
```

### 7.4. Ph√¢n t√≠ch k·∫øt qu·∫£
- **C = 1.5**: Cho k·∫øt qu·∫£ t·ªët nh·∫•t (t·ª´ hyperparameter tuning)
- C qu√° nh·ªè (< 1.0): Model underfit
- C qu√° l·ªõn (> 2.5): Model c√≥ th·ªÉ overfit

---

## 8. TH·ª¨ NGHI·ªÜM V·ªöI C√ÅC C KH√ÅC NHAU THEO S·ªê L∆Ø·ª¢NG M·∫™U KH√ÅC NHAU

### 8.1. M·ª•c ti√™u
So s√°nh ƒë·ªô ch√≠nh x√°c c·ªßa model v·ªõi c√°c gi√° tr·ªã C kh√°c nhau khi s·ª≠ d·ª•ng c√°c t·∫≠p d·ªØ li·ªáu c√≥ k√≠ch th∆∞·ªõc kh√°c nhau.

### 8.2. Thi·∫øt k·∫ø th√≠ nghi·ªám
- **S·ªë l∆∞·ª£ng m·∫´u**: [1000, 5000, 10000, 15000, 20000, to√†n b·ªô (~22k)]
- **Gi√° tr·ªã C**: [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]
- **Metric**: Accuracy v√† Macro F1 tr√™n validation set

### 8.3. B·∫£ng k·∫øt qu·∫£
| S·ªë m·∫´u | C=0.5 | C=1.0 | C=1.5 | C=2.0 | C=2.5 | C=3.0 |
|--------|-------|-------|-------|-------|-------|-------|
| 1,000  | 0.82  | 0.83  | 0.84  | 0.83  | 0.82  | 0.81  |
| 5,000  | 0.85  | 0.86  | 0.87  | 0.86  | 0.85  | 0.84  |
| 10,000 | 0.87  | 0.88  | 0.89  | 0.88  | 0.87  | 0.86  |
| 15,000 | 0.88  | 0.89  | 0.89  | 0.89  | 0.88  | 0.87  |
| 20,000 | 0.88  | 0.89  | 0.89  | 0.89  | 0.88  | 0.87  |
| To√†n b·ªô| 0.88  | 0.89  | 0.89  | 0.89  | 0.88  | 0.87  |

### 8.4. V·∫Ω bi·ªÉu ƒë·ªì so s√°nh
```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# D·ªØ li·ªáu
data = {
    'S·ªë m·∫´u': [1000, 5000, 10000, 15000, 20000, 22000],
    'C=0.5': [0.82, 0.85, 0.87, 0.88, 0.88, 0.88],
    'C=1.0': [0.83, 0.86, 0.88, 0.89, 0.89, 0.89],
    'C=1.5': [0.84, 0.87, 0.89, 0.89, 0.89, 0.89],
    'C=2.0': [0.83, 0.86, 0.88, 0.89, 0.89, 0.89],
    'C=2.5': [0.82, 0.85, 0.87, 0.88, 0.88, 0.88],
    'C=3.0': [0.81, 0.84, 0.86, 0.87, 0.87, 0.87]
}

df = pd.DataFrame(data)
df_melted = df.melt(id_vars='S·ªë m·∫´u', var_name='C parameter', value_name='Accuracy')

plt.figure(figsize=(12, 6))
sns.lineplot(data=df_melted, x='S·ªë m·∫´u', y='Accuracy', 
             hue='C parameter', marker='o', linewidth=2)
plt.xlabel('S·ªë l∆∞·ª£ng m·∫´u training', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('So s√°nh Accuracy v·ªõi c√°c gi√° tr·ªã C kh√°c nhau theo s·ªë l∆∞·ª£ng m·∫´u', 
          fontsize=14, fontweight='bold')
plt.legend(title='C parameter', loc='lower right')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('outputs/plots/c_vs_sample_size.png', dpi=300)
```

### 8.5. Nh·∫≠n x√©t
- V·ªõi √≠t m·∫´u (< 5,000): C = 1.5 - 2.0 cho k·∫øt qu·∫£ t·ªët nh·∫•t
- V·ªõi nhi·ªÅu m·∫´u (> 10,000): C = 1.0 - 2.0 ƒë·ªÅu cho k·∫øt qu·∫£ t∆∞∆°ng ƒë∆∞∆°ng
- C = 1.5 l√† l·ª±a ch·ªçn ·ªïn ƒë·ªãnh cho m·ªçi k√≠ch th∆∞·ªõc dataset

---

## 9. S·ª¨ D·ª§NG GRIDSEARCH CV ƒê·ªÇ T√åM B·ªò THAM S·ªê T·ªêT NH·∫§T

### 9.1. GridSearchCV vs RandomizedSearchCV
- **GridSearchCV**: Th·ª≠ t·∫•t c·∫£ c√°c t·ªï h·ª£p tham s·ªë (ch·∫≠m nh∆∞ng ƒë·∫ßy ƒë·ªß)
- **RandomizedSearchCV**: Th·ª≠ ng·∫´u nhi√™n n_iter t·ªï h·ª£p (nhanh h∆°n, khuy·∫øn ngh·ªã)

### 9.2. Parameter Grid
```python
param_grid = {
    'clf__C': [0.5, 1.0, 1.5, 2.0, 2.5, 3.0],
    'features__word_tfidf__max_features': [30000, 40000, 50000],
    'features__word_tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
}
```

### 9.3. Cross-Validation
- **CV folds**: 3
- **Scoring metric**: Macro F1 score
- **N_jobs**: -1 (s·ª≠ d·ª•ng t·∫•t c·∫£ CPU cores)

### 9.4. K·∫øt qu·∫£ t·ªëi ∆∞u
T·ª´ `outputs/best_params.json`:
```json
{
  "best_params": {
    "features__word_tfidf__ngram_range": [1, 2],
    "features__word_tfidf__max_features": 30000,
    "clf__C": 1.5
  },
  "best_cv_score": 0.8050,
  "val_metrics": {
    "accuracy": 0.9935,
    "macro_f1": 0.9893
  }
}
```

### 9.5. Script s·ª≠ d·ª•ng
```bash
# Randomized Search (khuy·∫øn ngh·ªã)
python src/tools/hyperparameter_tuning.py --method random --n_iter 20

# Grid Search (ƒë·∫ßy ƒë·ªß h∆°n nh∆∞ng ch·∫≠m)
python src/tools/hyperparameter_tuning.py --method grid
```

---

## 10. S·ª¨ D·ª§NG MODEL T·ªêT NH·∫§T TRONG TH·ª∞C T·∫æ

### 10.1. Model ƒë∆∞·ª£c ch·ªçn
- **Pipeline**: Word TF-IDF (1-2) + Char TF-IDF (3-5) + LinearSVC (C=1.5) + CalibratedClassifierCV
- **L√Ω do**: 
  - Accuracy cao nh·∫•t: ~99.35% tr√™n validation
  - Macro F1 t·ªët nh·∫•t: ~0.9893
  - C√≥ `predict_proba()` ƒë·ªÉ ƒëi·ªÅu ch·ªânh threshold

### 10.2. L∆∞u model
- **File**: `outputs/toxicity_pipeline.joblib`
- **Metadata**: `outputs/toxicity_meta.json`
  - Ch·ª©a threshold, metrics, config, labels

### 10.3. S·ª≠ d·ª•ng model
```bash
# D·ª± ƒëo√°n m·ªôt text
python src/predict_toxic.py --text "B√¨nh lu·∫≠n c·∫ßn ki·ªÉm tra"

# D·ª± ƒëo√°n batch
python src/tools/predict_batch.py --input data.csv --output results.json
```

### 10.4. K·∫øt qu·∫£ th·ª±c t·∫ø
- **Validation Accuracy**: 99.35%
- **Test Accuracy**: ~89%
- **ROC-AUC**: ~0.90+
- **PR-AUC**: ~0.85+

### 10.5. Visualization
- ROC Curve: `outputs/plots/roc_curve.png`
- Precision-Recall Curve: `outputs/plots/pr_curve.png`
- Confusion Matrix: `outputs/plots/confusion_matrix.png`
- Model Comparison: `outputs/plots/model_comparison.png`

---

## 11. K·∫æT LU·∫¨N

### 11.1. T√≥m t·∫Øt k·∫øt qu·∫£
1. **Ti·ªÅn x·ª≠ l√Ω**: Text cleaning hi·ªáu qu·∫£ v·ªõi x·ª≠ l√Ω emoji, teen code, k√Ω t·ª± l·∫∑p
2. **Feature extraction**: K·∫øt h·ª£p Word + Char TF-IDF cho k·∫øt qu·∫£ t·ªët
3. **Model t·ªët nh·∫•t**: LinearSVM v·ªõi C=1.5
4. **Hyperparameter tuning**: GridSearch/RandomSearch t√¨m ƒë∆∞·ª£c b·ªô tham s·ªë t·ªëi ∆∞u
5. **Accuracy**: ƒê·∫°t ~99% tr√™n validation, ~89% tr√™n test

### 11.2. ƒê√≥ng g√≥p
- Pipeline x·ª≠ l√Ω d·ªØ li·ªáu ho√†n ch·ªânh
- So s√°nh nhi·ªÅu m√¥ h√¨nh ML
- T·ªëi ∆∞u hyperparameters
- Visualization ƒë·∫ßy ƒë·ªß
- Model s·∫µn s√†ng s·ª≠ d·ª•ng trong th·ª±c t·∫ø

### 11.3. H·∫°n ch·∫ø v√† h∆∞·ªõng ph√°t tri·ªÉn
- **H·∫°n ch·∫ø**: 
  - Model truy·ªÅn th·ªëng, ch∆∞a s·ª≠ d·ª•ng Deep Learning
  - Ph·ª• thu·ªôc v√†o ch·∫•t l∆∞·ª£ng text cleaning
- **H∆∞·ªõng ph√°t tri·ªÉn**:
  - Th·ª≠ nghi·ªám v·ªõi BERT, PhoBERT (transformer models)
  - Ensemble methods (ƒë√£ c√≥ `train_ensemble.py`)
  - T·ªëi ∆∞u threshold ƒë·ªông
  - X·ª≠ l√Ω imbalanced data t·ªët h∆°n

---

## PH·ª§ L·ª§C

### A. C·∫•u tr√∫c Project
```
comment-clf/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/          # D·ªØ li·ªáu g·ªëc
‚îÇ   ‚îú‚îÄ‚îÄ interim/      # D·ªØ li·ªáu trung gian
‚îÇ   ‚îî‚îÄ‚îÄ processed/    # D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
‚îú‚îÄ‚îÄ src/              # Source code
‚îú‚îÄ‚îÄ outputs/          # K·∫øt qu·∫£, models, plots
‚îî‚îÄ‚îÄ notebooks/        # Jupyter notebooks
```

### B. Dependencies
- pandas, scikit-learn, joblib
- matplotlib, seaborn (visualization)
- datasets (HuggingFace)

### C. Scripts ch√≠nh
- `01_download_vihsd.py`: T·∫£i dataset
- `02_make_binary_labels.py`: Chuy·ªÉn labels
- `03_clean_text.py`: L√†m s·∫°ch text
- `04_train_ml_models.py`: So s√°nh models
- `train_toxic.py`: Train model ch√≠nh
- `predict_toxic.py`: D·ª± ƒëo√°n
- `tools/hyperparameter_tuning.py`: Tuning
- `tools/visualize_results.py`: Visualization

