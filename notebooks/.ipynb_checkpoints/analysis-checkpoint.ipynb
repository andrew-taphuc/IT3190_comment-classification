{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ph√¢n t√≠ch v√† Train Model Toxic Comment Classification\n",
    "\n",
    "Notebook n√†y s·ª≠ d·ª•ng d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a t·ª´ `data/processed/` ƒë·ªÉ:\n",
    "- Ph√¢n t√≠ch d·ªØ li·ªáu (EDA)\n",
    "- Train v√† ƒë√°nh gi√° model\n",
    "- D·ª± ƒëo√°n comment toxic/non-toxic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    f1_score, \n",
    "    accuracy_score,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "\n",
    "# Import h√†m l√†m s·∫°ch text\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from text_cleaner import clean_text\n",
    "\n",
    "# Thi·∫øt l·∫≠p style cho plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ ƒê√£ import t·∫•t c·∫£ th∆∞ vi·ªán c·∫ßn thi·∫øt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load d·ªØ li·ªáu t·ª´ data/processed\n",
    "data_dir = Path('../data/processed')\n",
    "\n",
    "train_df = pd.read_csv(data_dir / 'train.csv')\n",
    "val_df = pd.read_csv(data_dir / 'val.csv')\n",
    "test_df = pd.read_csv(data_dir / 'test.csv')\n",
    "\n",
    "print(f\"üìä K√≠ch th∆∞·ªõc d·ªØ li·ªáu:\")\n",
    "print(f\"  Train: {len(train_df):,} samples\")\n",
    "print(f\"  Val:   {len(val_df):,} samples\")\n",
    "print(f\"  Test:  {len(test_df):,} samples\")\n",
    "print(f\"\\nüìã Columns: {list(train_df.columns)}\")\n",
    "\n",
    "# Hi·ªÉn th·ªã v√†i m·∫´u\n",
    "print(\"\\nüîç M·∫´u d·ªØ li·ªáu train:\")\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ph√¢n t√≠ch d·ªØ li·ªáu (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ph√¢n b·ªë label\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (name, df) in enumerate([('Train', train_df), ('Val', val_df), ('Test', test_df)]):\n",
    "    label_counts = df['label'].value_counts()\n",
    "    axes[idx].bar(label_counts.index, label_counts.values, color=['#2ecc71', '#e74c3c'])\n",
    "    axes[idx].set_title(f'{name} Set - Label Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Label')\n",
    "    axes[idx].set_ylabel('Count')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Th√™m s·ªë li·ªáu tr√™n c·ªôt\n",
    "    for i, (label, count) in enumerate(label_counts.items()):\n",
    "        axes[idx].text(i, count + max(label_counts) * 0.01, f'{count:,}', \n",
    "                       ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # T√≠nh ph·∫ßn trƒÉm\n",
    "    total = len(df)\n",
    "    for i, (label, count) in enumerate(label_counts.items()):\n",
    "        pct = count / total * 100\n",
    "        axes[idx].text(i, count / 2, f'{pct:.1f}%', \n",
    "                      ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# In th·ªëng k√™ chi ti·∫øt\n",
    "print(\"\\nüìà Th·ªëng k√™ ph√¢n b·ªë label:\")\n",
    "for name, df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(df['label'].value_counts())\n",
    "    print(f\"T·ª∑ l·ªá: {df['label'].value_counts(normalize=True).to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ph√¢n t√≠ch ƒë·ªô d√†i text\n",
    "train_df['text_length'] = train_df['text'].str.len()\n",
    "val_df['text_length'] = val_df['text'].str.len()\n",
    "test_df['text_length'] = test_df['text'].str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (name, df) in enumerate([('Train', train_df), ('Val', val_df), ('Test', test_df)]):\n",
    "    axes[idx].hist(df['text_length'], bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].axvline(df['text_length'].mean(), color='red', linestyle='--', \n",
    "                      label=f'Mean: {df[\"text_length\"].mean():.1f}')\n",
    "    axes[idx].axvline(df['text_length'].median(), color='green', linestyle='--', \n",
    "                      label=f'Median: {df[\"text_length\"].median():.1f}')\n",
    "    axes[idx].set_title(f'{name} Set - Text Length Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Text Length (characters)')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìè Th·ªëng k√™ ƒë·ªô d√†i text:\")\n",
    "for name, df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Mean:   {df['text_length'].mean():.1f}\")\n",
    "    print(f\"  Median: {df['text_length'].median():.1f}\")\n",
    "    print(f\"  Min:    {df['text_length'].min()}\")\n",
    "    print(f\"  Max:    {df['text_length'].max()}\")\n",
    "    print(f\"  Std:    {df['text_length'].std():.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So s√°nh ƒë·ªô d√†i text gi·ªØa toxic v√† non-toxic\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (name, df) in enumerate([('Train', train_df), ('Val', val_df), ('Test', test_df)]):\n",
    "    toxic_lengths = df[df['label'] == 'toxic']['text_length']\n",
    "    non_toxic_lengths = df[df['label'] == 'non_toxic']['text_length']\n",
    "    \n",
    "    axes[idx].hist([non_toxic_lengths, toxic_lengths], bins=30, \n",
    "                   label=['non_toxic', 'toxic'], \n",
    "                   color=['#2ecc71', '#e74c3c'], \n",
    "                   alpha=0.7, edgecolor='black')\n",
    "    axes[idx].set_title(f'{name} Set - Text Length by Label', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Text Length (characters)')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä So s√°nh ƒë·ªô d√†i text theo label:\")\n",
    "for name, df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  non_toxic - Mean: {df[df['label']=='non_toxic']['text_length'].mean():.1f}\")\n",
    "    print(f\"  toxic     - Mean: {df[df['label']=='toxic']['text_length'].mean():.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chu·∫©n b·ªã d·ªØ li·ªáu\n",
    "X_train = train_df['text'].values\n",
    "y_train = train_df['label'].values\n",
    "X_val = val_df['text'].values\n",
    "y_val = val_df['label'].values\n",
    "X_test = test_df['text'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ chu·∫©n b·ªã d·ªØ li·ªáu:\")\n",
    "print(f\"  X_train: {len(X_train):,} samples\")\n",
    "print(f\"  X_val:   {len(X_val):,} samples\")\n",
    "print(f\"  X_test:  {len(X_test):,} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X√¢y d·ª±ng pipeline t∆∞∆°ng t·ª± train_toxic.py\n",
    "def build_pipeline(C=2.0):\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "      - word TF-IDF (1-2)\n",
    "      - char TF-IDF (3-5)\n",
    "      - FeatureUnion\n",
    "      - LinearSVC (balanced)\n",
    "      - CalibratedClassifierCV => predict_proba (score)\n",
    "    \"\"\"\n",
    "    word_tfidf = TfidfVectorizer(\n",
    "        preprocessor=clean_text,\n",
    "        analyzer=\"word\",\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        sublinear_tf=True,\n",
    "    )\n",
    "\n",
    "    char_tfidf = TfidfVectorizer(\n",
    "        preprocessor=clean_text,\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(3, 5),\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "    )\n",
    "\n",
    "    feats = FeatureUnion(\n",
    "        [\n",
    "            (\"word_tfidf\", word_tfidf),\n",
    "            (\"char_tfidf\", char_tfidf),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    base_svm = LinearSVC(C=C, class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "    clf = CalibratedClassifierCV(\n",
    "        estimator=base_svm,\n",
    "        method=\"sigmoid\",\n",
    "        cv=3,\n",
    "    )\n",
    "\n",
    "    return Pipeline(\n",
    "        [\n",
    "            (\"features\", feats),\n",
    "            (\"clf\", clf),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# T·∫°o pipeline\n",
    "pipe = build_pipeline(C=2.0)\n",
    "print(\"‚úÖ ƒê√£ t·∫°o pipeline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"üöÄ B·∫Øt ƒë·∫ßu training...\")\n",
    "pipe.fit(X_train, y_train)\n",
    "print(\"‚úÖ Training ho√†n t·∫•t!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ƒê√°nh gi√° Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê√°nh gi√° tr√™n validation set\n",
    "y_val_pred = pipe.predict(X_val)\n",
    "y_val_proba = pipe.predict_proba(X_val)\n",
    "\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "val_f1_macro = f1_score(y_val, y_val_pred, average='macro')\n",
    "val_f1_weighted = f1_score(y_val, y_val_pred, average='weighted')\n",
    "\n",
    "print(\"üìä K·∫øt qu·∫£ tr√™n Validation Set:\")\n",
    "print(f\"  Accuracy:        {val_acc:.4f}\")\n",
    "print(f\"  F1 (macro):     {val_f1_macro:.4f}\")\n",
    "print(f\"  F1 (weighted):  {val_f1_weighted:.4f}\")\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê√°nh gi√° tr√™n test set\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "y_test_proba = pipe.predict_proba(X_test)\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_f1_macro = f1_score(y_test, y_test_pred, average='macro')\n",
    "test_f1_weighted = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(\"üìä K·∫øt qu·∫£ tr√™n Test Set:\")\n",
    "print(f\"  Accuracy:        {test_acc:.4f}\")\n",
    "print(f\"  F1 (macro):     {test_f1_macro:.4f}\")\n",
    "print(f\"  F1 (weighted):  {test_f1_weighted:.4f}\")\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Validation set\n",
    "cm_val = confusion_matrix(y_val, y_val_pred)\n",
    "sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=pipe.classes_, yticklabels=pipe.classes_)\n",
    "axes[0].set_title('Confusion Matrix - Validation Set', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Test set\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=pipe.classes_, yticklabels=pipe.classes_)\n",
    "axes[1].set_title('Confusion Matrix - Test Set', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (name, y_true, y_proba) in enumerate([\n",
    "    ('Validation', y_val, y_val_proba),\n",
    "    ('Test', y_test, y_test_proba)\n",
    "]):\n",
    "    # L·∫•y index c·ªßa class 'toxic'\n",
    "    toxic_idx = list(pipe.classes_).index('toxic')\n",
    "    y_scores = y_proba[:, toxic_idx]\n",
    "    \n",
    "    # T√≠nh ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true == 'toxic', y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    axes[idx].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                   label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    axes[idx].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    axes[idx].set_xlim([0.0, 1.0])\n",
    "    axes[idx].set_ylim([0.0, 1.05])\n",
    "    axes[idx].set_xlabel('False Positive Rate', fontsize=11)\n",
    "    axes[idx].set_ylabel('True Positive Rate', fontsize=11)\n",
    "    axes[idx].set_title(f'ROC Curve - {name} Set', fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend(loc=\"lower right\")\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. D·ª± ƒëo√°n v√† Ph√¢n t√≠ch Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H√†m ƒë·ªÉ d·ª± ƒëo√°n v√† hi·ªÉn th·ªã k·∫øt qu·∫£\n",
    "def predict_and_display(text, threshold=0.70):\n",
    "    \"\"\"D·ª± ƒëo√°n v√† hi·ªÉn th·ªã k·∫øt qu·∫£ cho m·ªôt text\"\"\"\n",
    "    proba = pipe.predict_proba([text])[0]\n",
    "    classes = list(pipe.classes_)\n",
    "    toxic_idx = classes.index('toxic')\n",
    "    toxic_score = proba[toxic_idx]\n",
    "    label = 'toxic' if toxic_score >= threshold else 'non_toxic'\n",
    "    \n",
    "    print(f\"üìù Text: \\\"{text}\\\"\")\n",
    "    print(f\"üè∑Ô∏è  Label: {label}\")\n",
    "    print(f\"üìä Toxic Score: {toxic_score:.4f}\")\n",
    "    print(f\"üìä Non-toxic Score: {proba[1-toxic_idx]:.4f}\")\n",
    "    print(f\"‚öñÔ∏è  Threshold: {threshold}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'label': label,\n",
    "        'toxic_score': toxic_score,\n",
    "        'threshold': threshold\n",
    "    }\n",
    "\n",
    "# Test v·ªõi m·ªôt s·ªë examples\n",
    "test_examples = [\n",
    "    \"ƒë√¢y l√† m·ªôt comment b√¨nh th∆∞·ªùng\",\n",
    "    \"ƒëm m√†y ngu vcl\",\n",
    "    \"b√†i vi·∫øt n√†y r·∫•t hay v√† h·ªØu √≠ch\",\n",
    "    \"cmm th·∫±ng n√†y ngu qu√°\",\n",
    "    \"c·∫£m ∆°n b·∫°n ƒë√£ chia s·∫ª\",\n",
    "]\n",
    "\n",
    "print(\"üîç D·ª± ƒëo√°n m·ªôt s·ªë examples:\\n\")\n",
    "for example in test_examples:\n",
    "    predict_and_display(example)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ph√¢n t√≠ch c√°c tr∆∞·ªùng h·ª£p d·ª± ƒëo√°n sai\n",
    "print(\"üîç Ph√¢n t√≠ch False Positives v√† False Negatives tr√™n Test Set:\\n\")\n",
    "\n",
    "# False Positives: D·ª± ƒëo√°n toxic nh∆∞ng th·ª±c t·∫ø non_toxic\n",
    "fp_mask = (y_test == 'non_toxic') & (y_test_pred == 'toxic')\n",
    "fp_indices = np.where(fp_mask)[0]\n",
    "\n",
    "print(f\"‚ùå False Positives (D·ª± ƒëo√°n toxic nh∆∞ng th·ª±c t·∫ø non_toxic): {len(fp_indices)}\")\n",
    "if len(fp_indices) > 0:\n",
    "    print(\"\\nM·ªôt s·ªë v√≠ d·ª•:\")\n",
    "    for i, idx in enumerate(fp_indices[:5]):\n",
    "        print(f\"\\n{i+1}. Text: \\\"{X_test[idx]}\\\"\")\n",
    "        print(f\"   Toxic Score: {y_test_proba[idx, list(pipe.classes_).index('toxic')]:.4f}\")\n",
    "\n",
    "# False Negatives: D·ª± ƒëo√°n non_toxic nh∆∞ng th·ª±c t·∫ø toxic\n",
    "fn_mask = (y_test == 'toxic') & (y_test_pred == 'non_toxic')\n",
    "fn_indices = np.where(fn_mask)[0]\n",
    "\n",
    "print(f\"\\n\\n‚ùå False Negatives (D·ª± ƒëo√°n non_toxic nh∆∞ng th·ª±c t·∫ø toxic): {len(fn_indices)}\")\n",
    "if len(fn_indices) > 0:\n",
    "    print(\"\\nM·ªôt s·ªë v√≠ d·ª•:\")\n",
    "    for i, idx in enumerate(fn_indices[:5]):\n",
    "        print(f\"\\n{i+1}. Text: \\\"{X_test[idx]}\\\"\")\n",
    "        print(f\"   Toxic Score: {y_test_proba[idx, list(pipe.classes_).index('toxic')]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. L∆∞u Model (T√πy ch·ªçn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L∆∞u model v√† metadata\n",
    "model_path = '../toxicity_pipeline.joblib'\n",
    "meta_path = '../toxicity_meta.json'\n",
    "\n",
    "# L∆∞u model\n",
    "joblib.dump(pipe, model_path)\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u model: {model_path}\")\n",
    "\n",
    "# L∆∞u metadata\n",
    "labels = sorted(list(set(list(y_train) + list(y_val) + list(y_test))))\n",
    "meta = {\n",
    "    \"labels\": labels,\n",
    "    \"threshold_toxic\": 0.70,\n",
    "    \"model_out\": model_path,\n",
    "    \"val_macro_f1\": float(val_f1_macro),\n",
    "    \"test_macro_f1\": float(test_f1_macro),\n",
    "    \"val_accuracy\": float(val_acc),\n",
    "    \"test_accuracy\": float(test_acc),\n",
    "    \"notes\": \"Pipeline: word+char TF-IDF + LinearSVC(class_weight=balanced) + CalibratedClassifierCV(sigmoid, cv=3)\",\n",
    "}\n",
    "\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u metadata: {meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. T√≥m t·∫Øt\n",
    "\n",
    "Notebook n√†y ƒë√£:\n",
    "- ‚úÖ Load v√† ph√¢n t√≠ch d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a\n",
    "- ‚úÖ Th·ª±c hi·ªán EDA (ph√¢n b·ªë label, ƒë·ªô d√†i text, v.v.)\n",
    "- ‚úÖ Train model v·ªõi pipeline TF-IDF + LinearSVC\n",
    "- ‚úÖ ƒê√°nh gi√° model tr√™n validation v√† test set\n",
    "- ‚úÖ Visualize k·∫øt qu·∫£ (confusion matrix, ROC curve)\n",
    "- ‚úÖ D·ª± ƒëo√°n v√† ph√¢n t√≠ch examples\n",
    "- ‚úÖ L∆∞u model v√† metadata\n",
    "\n",
    "**K·∫øt qu·∫£ ch√≠nh:** (xem c√°c cell tr√™n ƒë·ªÉ xem k·∫øt qu·∫£ chi ti·∫øt)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
